{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baisc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From torch\n",
    "import torch\n",
    "## nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "## optim\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "## utils\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "## torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls2id = {\"Happy\": 0, \"Sad\": 1, \"Fear\": 2}\n",
    "id2cls = [\"Happy\", \"Sad\", \"Fear\"]\n",
    "\n",
    "BATCHSIZE = 10\n",
    "PATH = \"aithon2020_level2_traning.csv\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definin functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(PATH):\n",
    "    data     = pd.read_csv(PATH)\n",
    "    try:\n",
    "        labels   = data[\"emotion\"]\n",
    "        data     = data.drop([\"emotion\"], axis = 1)\n",
    "    except:\n",
    "        labels = None\n",
    "    images   = np.array(data.values).reshape(len(data.values), 48, 48)\n",
    "    images   = images/255\n",
    "    return images, labels\n",
    "    \n",
    "def loader(PATH):\n",
    "    images, labels = load_data(PATH)\n",
    "    images = torch.tensor(images)\n",
    "    images = images.view(images.shape[0], -1, images.shape[1], images.shape[2])\n",
    "\n",
    "    if labels is not None:\n",
    "        target = []\n",
    "        for label in labels.values:\n",
    "            target.append(cls2id[label])\n",
    "        target = torch.tensor(target)\n",
    "    else:\n",
    "        target = None\n",
    "    \n",
    "    return images, target\n",
    "\n",
    "def data_split(X, Y, test_size, shuffle = True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_size, shuffle = shuffle)\n",
    "    return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "def create_batch(X, Y, batch_size = 1):\n",
    "    batch_x = [X[i: i + batch_size] for i in range(0, len(X), batch_size)]\n",
    "    batch_y = [Y[i: i + batch_size] for i in range(0, len(Y), batch_size)] \n",
    "    return list(zip(batch_x, batch_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_length      = round(0.7 * datalen)\n",
    "# test_length       = round(0.3 * datalen)\n",
    "\n",
    "## Loading images\n",
    "images, target = loader(PATH)\n",
    "## Train test split\n",
    "train_X, test_X, train_Y, test_Y = data_split(images, target, test_size = 0.3) \n",
    "## Train loader \n",
    "trainloader = create_batch(train_X, train_Y, batch_size = BATCHSIZE)\n",
    "## Test loader\n",
    "testloader = create_batch(test_X, test_Y, batch_size = BATCHSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used 3 types of model\n",
    "\n",
    "    1. RESNET101\n",
    "    2. VGG19\n",
    "    3. XGBOOST\n",
    "\n",
    "Finally used bagging to ensemble those models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient freezer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RESNET101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESNET(nn.Module):\n",
    "    def __init__(self, criterion = None, optimizer = None, learning_rate = 0.001, image_dimention = 1, categories = 3):\n",
    "        super(RESNET, self).__init__()\n",
    "        ## Defining networt\n",
    "         # Defaulf input image dimention is 1\n",
    "         # Default output categories is 3\n",
    "        self.pretrained = models.resnet101(pretrained = True)\n",
    "        self.pretrained.conv1 = nn.Conv2d(image_dimention, 64, kernel_size = (3, 3), stride=(2,2), padding=(3,3), bias=False)\n",
    "        num_ftrs = self.pretrained.fc.in_features\n",
    "        self.pretrained.fc = nn.Linear(num_ftrs, categories)\n",
    "        \n",
    "        ## Defining optimizer and loss function\n",
    "         # Default loss function is cross entropy\n",
    "         # Default optimizer is SGD\n",
    "         # Default learning rate is 0.001\n",
    "        if criterion:\n",
    "            self.criterion = criterion\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = optim.SGD(self.pretrained.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def train(self, traindata, valdata = None, numberEpoch = 10, DEBUG = True):\n",
    "        trainlen = sum(list(batch[0].shape[0] for batch in traindata))\n",
    "        total_batch = len(traindata)\n",
    "        ## Loop over the dataset multiple times\n",
    "        for epoch in range(numberEpoch): \n",
    "            running_corrects = 0.0\n",
    "            running_loss     = 0.0\n",
    "            if DEBUG:\n",
    "                pbar = tqdm(enumerate(traindata, 0), total = total_batch, desc = \"Loss 0, Completed\", ncols = 800)\n",
    "            if not DEBUG:\n",
    "                pbar = enumerate(traindata, 0)\n",
    "            for count, data in pbar:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                batch  = inputs.shape[0]\n",
    "                inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "                \n",
    "                ## zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                ## forward + backward + optimize\n",
    "                outputs = self.forward(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Calculating statistics\n",
    "                running_loss += loss.item() * batch\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                ## Showing statistics\n",
    "                if DEBUG:\n",
    "                    pbar.set_description(\"Loss %.3f, Completed\" %(running_loss/trainlen))\n",
    "            if DEBUG:\n",
    "                epoch_loss = running_loss/trainlen\n",
    "                epoch_acc  = running_corrects/trainlen\n",
    "                print('Epoch %d completed, average loss: %.3f, accuracy: %.3f' %(epoch + 1, epoch_loss, epoch_acc))\n",
    "            \n",
    "                if valdata:\n",
    "                    val_loss, val_acc = self.evaluate(valdata)\n",
    "                    print('Validation, average loss: %.3f, accuracy: %.3f' %(val_loss, val_acc))\n",
    "                \n",
    "    def evaluate(self, testdata):\n",
    "        running_corrects = 0.0\n",
    "        running_loss     = 0.0\n",
    "        testlen = sum(list(batch[0].shape[0] for batch in testdata))\n",
    "        for data in testdata:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            batch  = inputs.shape[0]\n",
    "            inputs = inputs.type(torch.cuda.FloatTensor)            \n",
    "            ## Forward\n",
    "            outputs = self.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            ## Loss and accuracy\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            running_loss += loss.item() * batch\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        loss = running_loss/testlen\n",
    "        acc  = running_corrects/testlen\n",
    "        return loss, acc\n",
    "        \n",
    "    def predict(self, testdata, ID = None):\n",
    "        predicted_labels = []\n",
    "        for data in testdata:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            batch  = inputs.shape[0]\n",
    "            inputs = inputs.type(torch.cuda.FloatTensor)            \n",
    "            ## Forward\n",
    "            outputs = self.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predicted_labels += preds.tolist()\n",
    "        if ID:\n",
    "            return([ID[label] for label in predicted_labels])\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNET(nn.Module):\n",
    "    def __init__(self, criterion = None, optimizer = None, learning_rate = 0.001, image_dimention = 1, categories = 3):\n",
    "        super(VGGNET, self).__init__()\n",
    "        ## Defining networt\n",
    "         # Defaulf input image dimention is 1\n",
    "         # Default output categories is 3\n",
    "        self.pretrained = models.vgg19(pretrained = True)\n",
    "        self.pretrained.features[0] = nn.Conv2d(image_dimention, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        num_ftrs = self.pretrained.classifier[6].in_features\n",
    "        self.pretrained.classifier[6] = nn.Linear(num_ftrs, categories)\n",
    "        \n",
    "        ## Defining optimizer and loss function\n",
    "         # Default loss function is cross entropy\n",
    "         # Default optimizer is SGD\n",
    "         # Default learning rate is 0.001\n",
    "        if criterion:\n",
    "            self.criterion = criterion\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = optim.SGD(self.pretrained.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def train(self, traindata, valdata = None, numberEpoch = 10, DEBUG = True):\n",
    "        \n",
    "        trainlen = sum(list(batch[0].shape[0] for batch in traindata))\n",
    "        total_batch = len(traindata)\n",
    "        ## Loop over the dataset multiple times\n",
    "        for epoch in range(numberEpoch): \n",
    "            running_corrects = 0.0\n",
    "            running_loss     = 0.0\n",
    "            if DEBUG:\n",
    "                pbar = tqdm(enumerate(traindata, 0), total = total_batch, desc = \"Loss 0, Completed\", ncols = 800)\n",
    "            else:\n",
    "                pbar = enumerate(traindata, 0)\n",
    "            for count, data in pbar:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                batch  = inputs.shape[0]\n",
    "                inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "                \n",
    "                ## zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                ## forward + backward + optimize\n",
    "                outputs = self.forward(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Calculating statistics\n",
    "                running_loss += loss.item() * batch\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                ## Showing statistics\n",
    "                if DEBUG:\n",
    "                    pbar.set_description(\"Loss %.3f, Completed\" %(running_loss/trainlen))\n",
    "            if DEBUG:\n",
    "                epoch_loss = running_loss/trainlen\n",
    "                epoch_acc  = running_corrects/trainlen\n",
    "                print('Epoch %d completed, average loss: %.3f, accuracy: %.3f' %(epoch + 1, epoch_loss, epoch_acc))\n",
    "            \n",
    "                if valdata:\n",
    "                    val_loss, val_acc = self.evaluate(valdata)\n",
    "                    print('Validation, average loss: %.3f, accuracy: %.3f' %(val_loss, val_acc))\n",
    "                \n",
    "    def evaluate(self, testdata):\n",
    "        running_corrects = 0.0\n",
    "        running_loss     = 0.0\n",
    "        testlen = sum(list(batch[0].shape[0] for batch in testdata))\n",
    "        with torch.no_grad():\n",
    "            for data in testdata:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                batch  = inputs.shape[0]\n",
    "                inputs = inputs.type(torch.cuda.FloatTensor)            \n",
    "                ## Forward\n",
    "                outputs = self.forward(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                ## Loss and accuracy\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item() * batch\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        loss = running_loss/testlen\n",
    "        acc  = running_corrects/testlen\n",
    "        return loss, acc\n",
    "        \n",
    "    def predict(self, testdata, ID = None):\n",
    "        predicted_labels = []\n",
    "        for data in testdata:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            batch  = inputs.shape[0]\n",
    "            inputs = inputs.type(torch.cuda.FloatTensor)            \n",
    "            ## Forward\n",
    "            outputs = self.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predicted_labels += preds.tolist()\n",
    "        if ID:\n",
    "            return([ID[label] for label in predicted_labels])\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB = XGBClassifier(max_depth = 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers::  10\n",
      "Layers Freezed  ::  0\n"
     ]
    }
   ],
   "source": [
    "model_resnet = RESNET()\n",
    "model_resnet = model_resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.train(trainloader, valdata = testloader, numberEpoch = 25, DEBUG = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = VGGNET()\n",
    "model_vgg = model_vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.train(trainloader, valdata = testloader, numberEpoch = 25, DEBUG = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resnet\n",
    "prediction_resnet = model_resnet.predict(testloader)\n",
    "## VGG\n",
    "prediction_vgg    = model_vgg.predict(testloader)\n",
    "## XGB\n",
    "prediction_xgb    = model_XGB.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
