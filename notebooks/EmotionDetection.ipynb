{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baisc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mode \n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From torch\n",
    "import torch\n",
    "## nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "## optim\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "## utils\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "## torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls2id = {\"Happy\": 0, \"Sad\": 1, \"Fear\": 2}\n",
    "id2cls = [\"Happy\", \"Sad\", \"Fear\"]\n",
    "\n",
    "BATCHSIZE = 50\n",
    "PATH = \"aithon2020_level2_traning.csv\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(PATH):\n",
    "    data     = pd.read_csv(PATH)\n",
    "    try:\n",
    "        labels   = data[\"emotion\"]\n",
    "        data     = data.drop([\"emotion\"], axis = 1)\n",
    "    except:\n",
    "        labels = None\n",
    "    images   = np.array(data.values).reshape(len(data.values), 48, 48)\n",
    "    images   = images/255\n",
    "    return images, labels\n",
    "    \n",
    "def loader(PATH):\n",
    "    images, labels = load_data(PATH)\n",
    "    images = torch.tensor(images)\n",
    "    images = images.view(images.shape[0], -1, images.shape[1], images.shape[2])\n",
    "\n",
    "    if labels is not None:\n",
    "        target = []\n",
    "        for label in labels.values:\n",
    "            target.append(cls2id[label])\n",
    "        target = torch.tensor(target)\n",
    "    else:\n",
    "        target = None\n",
    "    \n",
    "    return images, target\n",
    "\n",
    "def data_split(X, Y, test_size, shuffle = True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_size, shuffle = shuffle)\n",
    "    return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "def create_batch(X, Y, batch_size = 1):\n",
    "    batch_x = [X[i: i + batch_size] for i in range(0, len(X), batch_size)]\n",
    "    batch_y = [Y[i: i + batch_size] for i in range(0, len(Y), batch_size)] \n",
    "    return list(zip(batch_x, batch_y))\n",
    "\n",
    "def flatten_array(X):\n",
    "    X = X.reshape(X.shape[0], X.shape[2] * X.shape[3])\n",
    "    return X\n",
    "\n",
    "def ML_compatible(x_train, y_train, x_test = None, y_test = None):\n",
    "    x_train = flatten_array(x_train)\n",
    "    x_train = x_train.numpy()\n",
    "    y_train = y_train.numpy()\n",
    "    if x_test is not None:\n",
    "            x_test = flatten_array(x_test)\n",
    "            x_test = x_test.numpy()\n",
    "            y_test = y_test.numpy()\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def oversampling(x_train, y_train):\n",
    "    dic = {}\n",
    "    for i in range(len(y_train)):\n",
    "        try:\n",
    "            dic[y_train[i].tolist()].append(x_train[i])\n",
    "        except:  \n",
    "            dic[y_train[i].tolist()] = [x_train[i]]\n",
    "\n",
    "    max_occurance = max([len(dic[key]) for key in dic.keys()])\n",
    "\n",
    "    for key in dic.keys():\n",
    "        l = (max_occurance - len(dic[key]))\n",
    "        while l > 0:\n",
    "            x_train = torch.cat((x_train, random.choice(dic[key]).view(-1, 1, 48, 48)))\n",
    "            y_train = torch.cat((y_train, torch.tensor([key])))\n",
    "            l -= 1\n",
    "            \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_length      = round(0.7 * datalen)\n",
    "# test_length       = round(0.3 * datalen)\n",
    "\n",
    "## Loading images\n",
    "images, target = loader(PATH)\n",
    "## Train test split\n",
    "train_X, test_X, train_Y, test_Y = data_split(images, target, test_size = 0.3)\n",
    "## Oversampling\n",
    "train_X, train_Y = oversampling(train_X, train_Y)\n",
    "## Train loader \n",
    "trainloader = create_batch(train_X, train_Y, batch_size = BATCHSIZE)\n",
    "## Test loader\n",
    "testloader = create_batch(test_X, test_Y, batch_size = BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_ML , train_Y_ML, test_X_ML, test_Y_ML = ML_compatible(train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used 3 types of model\n",
    "\n",
    "    1. RESNET101\n",
    "    2. VGG19\n",
    "    3. XGBOOST\n",
    "\n",
    "Finally used bagging to ensemble those models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient freezer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RESNET101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESNET(nn.Module):\n",
    "    def __init__(self, criterion = None, optimizer = None, learning_rate = 0.001, image_dimention = 1, categories = 3):\n",
    "        super(RESNET, self).__init__()\n",
    "        ## Defining networt\n",
    "         # Defaulf input image dimention is 1\n",
    "         # Default output categories is 3\n",
    "        self.pretrained = models.resnext101_32x8d(pretrained = True)\n",
    "        self.pretrained.conv1 = nn.Conv2d(image_dimention, 64, kernel_size = (3, 3), stride=(2,2), padding=(3,3), bias=False)\n",
    "        num_ftrs = self.pretrained.fc.in_features\n",
    "        self.pretrained.fc = nn.Linear(num_ftrs, categories)\n",
    "        \n",
    "        ## Defining optimizer and loss function\n",
    "         # Default loss function is cross entropy\n",
    "         # Default optimizer is SGD\n",
    "         # Default learning rate is 0.001\n",
    "        if criterion:\n",
    "            self.criterion = criterion\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = optim.SGD(self.pretrained.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def train(self, traindata, valdata = None, numberEpoch = 10, DEBUG = True):\n",
    "        trainlen = sum(list(batch[0].shape[0] for batch in traindata))\n",
    "        total_batch = len(traindata)\n",
    "        ## Loop over the dataset multiple times\n",
    "        for epoch in range(numberEpoch): \n",
    "            running_corrects = 0.0\n",
    "            running_loss     = 0.0\n",
    "            if DEBUG:\n",
    "                pbar = tqdm(enumerate(traindata, 0), total = total_batch, desc = \"Loss 0, Completed\", ncols = 800)\n",
    "            else:\n",
    "                pbar = enumerate(traindata, 0)\n",
    "            for count, data in pbar:\n",
    "                inputs, labels = data[0], data[1]\n",
    "                inputs = inputs.type(torch.FloatTensor)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                batch  = inputs.shape[0]\n",
    "                \n",
    "                ## zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                ## forward + backward + optimize\n",
    "                outputs = self.forward(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Calculating statistics\n",
    "                running_loss += loss.item() * batch\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                ## Showing statistics\n",
    "                if DEBUG:\n",
    "                    pbar.set_description(\"Loss %.3f, Completed\" %(running_loss/trainlen))\n",
    "                    \n",
    "            if DEBUG:\n",
    "                epoch_loss = running_loss/trainlen\n",
    "                epoch_acc  = running_corrects/trainlen\n",
    "                print('Epoch %d completed, average loss: %.3f, accuracy: %.3f' %(epoch + 1, epoch_loss, epoch_acc))\n",
    "            \n",
    "                if valdata:\n",
    "                    val_loss, val_acc = self.evaluate(valdata)\n",
    "                    print('Validation, average loss: %.3f, accuracy: %.3f' %(val_loss, val_acc))\n",
    "                \n",
    "    def evaluate(self, testdata):\n",
    "        running_corrects = 0.0\n",
    "        running_loss     = 0.0\n",
    "        testlen = sum(list(batch[0].shape[0] for batch in testdata))\n",
    "        for data in testdata:\n",
    "            inputs, labels = data[0], data[1]\n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            batch  = inputs.shape[0]          \n",
    "            ## Forward\n",
    "            outputs = self.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            ## Loss and accuracy\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            running_loss += loss.item() * batch\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        loss = running_loss/testlen\n",
    "        acc  = running_corrects/testlen\n",
    "        return loss, acc\n",
    "        \n",
    "    def predict(self, testdata, ID = None):\n",
    "        predicted_labels = []\n",
    "        for data in testdata:\n",
    "            inputs, labels = data[0], data[1]\n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            batch  = inputs.shape[0]          \n",
    "            ## Forward\n",
    "            outputs = self.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predicted_labels += preds.tolist()\n",
    "        if ID:\n",
    "            return([ID[label] for label in predicted_labels])\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNET(nn.Module):\n",
    "    def __init__(self, criterion = None, optimizer = None, learning_rate = 0.001, image_dimention = 1, categories = 3):\n",
    "        super(VGGNET, self).__init__()\n",
    "        ## Defining networt\n",
    "         # Defaulf input image dimention is 1\n",
    "         # Default output categories is 3\n",
    "        self.pretrained = models.vgg16(pretrained = True)\n",
    "        self.pretrained.features[0] = nn.Conv2d(image_dimention, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        num_ftrs = self.pretrained.classifier[6].in_features\n",
    "        self.pretrained.classifier[6] = nn.Linear(num_ftrs, categories)\n",
    "        \n",
    "        ## Defining optimizer and loss function\n",
    "         # Default loss function is cross entropy\n",
    "         # Default optimizer is SGD\n",
    "         # Default learning rate is 0.001\n",
    "        if criterion:\n",
    "            self.criterion = criterion\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = optim.SGD(self.pretrained.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "#         self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size = 3, gamma = 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def train(self, traindata, valdata = None, numberEpoch = 10, DEBUG = True):\n",
    "        \n",
    "        trainlen = sum(list(batch[0].shape[0] for batch in traindata))\n",
    "        total_batch = len(traindata)\n",
    "        ## Loop over the dataset multiple times\n",
    "        for epoch in range(numberEpoch): \n",
    "            running_corrects = 0.0\n",
    "            running_loss     = 0.0\n",
    "            if DEBUG:\n",
    "                pbar = tqdm(enumerate(traindata, 0), total = total_batch, desc = \"Loss 0, Completed\", ncols = 800)\n",
    "            else:\n",
    "                pbar = enumerate(traindata, 0)\n",
    "            for count, data in pbar:\n",
    "                inputs, labels = data[0], data[1]\n",
    "                inputs = inputs.type(torch.FloatTensor)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                batch  = inputs.shape[0]\n",
    "                \n",
    "                ## zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                ## forward + backward + optimize\n",
    "                outputs = self.forward(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Calculating statistics\n",
    "                running_loss += loss.item() * batch\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                ## Learning rate scheduler\n",
    "#                 self.scheduler.step()\n",
    "                \n",
    "                ## Showing statistics\n",
    "                if DEBUG:\n",
    "                    pbar.set_description(\"Loss %.3f, Completed\" %(running_loss/trainlen))\n",
    "            if DEBUG:\n",
    "                epoch_loss = running_loss/trainlen\n",
    "                epoch_acc  = running_corrects/trainlen\n",
    "                print('Epoch %d completed, average loss: %.3f, accuracy: %.3f' %(epoch + 1, epoch_loss, epoch_acc))\n",
    "            \n",
    "                if valdata:\n",
    "                    val_loss, val_acc = self.evaluate(valdata)\n",
    "                    print('Validation, average loss: %.3f, accuracy: %.3f' %(val_loss, val_acc))\n",
    "                \n",
    "    def evaluate(self, testdata):\n",
    "        running_corrects = 0.0\n",
    "        running_loss     = 0.0\n",
    "        testlen = sum(list(batch[0].shape[0] for batch in testdata))\n",
    "        with torch.no_grad():\n",
    "            for data in testdata:\n",
    "                inputs, labels = data[0], data[1]\n",
    "                inputs = inputs.type(torch.FloatTensor)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                batch  = inputs.shape[0]\n",
    "                ## Forward\n",
    "                outputs = self.forward(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                ## Loss and accuracy\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item() * batch\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        loss = running_loss/testlen\n",
    "        acc  = running_corrects/testlen\n",
    "        return loss, acc\n",
    "        \n",
    "    def predict(self, testdata, ID = None):\n",
    "        predicted_labels = []\n",
    "        for data in testdata:\n",
    "            inputs, labels = data[0], data[1]\n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            batch  = inputs.shape[0]           \n",
    "            ## Forward\n",
    "            outputs = self.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predicted_labels += preds.tolist()\n",
    "        if ID:\n",
    "            return([ID[label] for label in predicted_labels])\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB = XGBClassifier(max_depth = 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = RESNET()\n",
    "model_resnet = model_resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.train(trainloader, valdata = testloader, numberEpoch = 30, DEBUG = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = VGGNET()\n",
    "model_vgg = model_vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_vgg.train(trainloader, valdata = testloader, numberEpoch = 50, DEBUG = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=3000,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_XGB.fit(train_X_ML, train_Y_ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resnet\n",
    "prediction_resnet = model_resnet.predict(testloader)\n",
    "## VGG\n",
    "prediction_vgg    = model_vgg.predict(testloader)\n",
    "# XGB\n",
    "prediction_xgb    = model_XGB.predict(test_X_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagged_prediction(prediction_list):\n",
    "    combined_prediction = list(zip(*prediction_list))\n",
    "    final_prediction   = []\n",
    "    for pred in combined_prediction:\n",
    "        try:\n",
    "            final_prediction.append(mode(pred))\n",
    "        except:\n",
    "            final_prediction.append(pred[1])\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = bagged_prediction([prediction_resnet, prediction_vgg, prediction_xgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6617375231053605"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(prediction_resnet, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7045594577942083"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(prediction_vgg, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61090573012939"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(prediction_xgb, test_Y_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7097966728280961"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(final_prediction, test_Y_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.29505995454485e-05, tensor(1., device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet.evaluate(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14051205685305387, tensor(0.9472))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg.evaluate(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
