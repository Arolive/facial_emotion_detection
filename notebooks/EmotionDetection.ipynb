{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baisc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From torch\n",
    "import torch\n",
    "## nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "## optim\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "## utils\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "## torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subhasish/anaconda3/lib/python3.6/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls2id = {\"Happy\": 0, \"Sad\": 1, \"Fear\": 2}\n",
    "id2cls = [\"Happy\", \"Sad\", \"Fear\"]\n",
    "\n",
    "BATCHSIZE = 10\n",
    "PATH = \"../data/aithon2020_level2_traning.csv\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definin functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(PATH):\n",
    "    data     = pd.read_csv(PATH)\n",
    "    try:\n",
    "        labels   = data[\"emotion\"]\n",
    "        data     = data.drop([\"emotion\"], axis = 1)\n",
    "    except:\n",
    "        labels = None\n",
    "    images   = np.array(data.values).reshape(len(data.values), 48, 48)\n",
    "    images   = images/255\n",
    "    return images, labels\n",
    "    \n",
    "def loader(PATH):\n",
    "    images, labels = load_data(PATH)\n",
    "    images = torch.tensor(images)\n",
    "    images = images.view(images.shape[0], -1, images.shape[1], images.shape[2])\n",
    "\n",
    "    if labels is not None:\n",
    "        target = []\n",
    "        for label in labels.values:\n",
    "            target.append(cls2id[label])\n",
    "        target = torch.tensor(target)\n",
    "    else:\n",
    "        target = None\n",
    "    \n",
    "    return images, target\n",
    "\n",
    "def data_split(X, Y, test_size, shuffle = True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_size, shuffle = shuffle)\n",
    "    return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "def create_batch(X, Y, batch_size = 1):\n",
    "    batch_x = [X[i: i + batch_size] for i in range(0, len(X), batch_size)]\n",
    "    batch_y = [Y[i: i + batch_size] for i in range(0, len(Y), batch_size)] \n",
    "    return list(zip(batch_x, batch_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "take(): argument 'index' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5a91f10e88aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## Train test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m## Train loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCHSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c1038716d83f>\u001b[0m in \u001b[0;36mdata_split\u001b[0;34m(X, Y, test_size, shuffle)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2102\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2102\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    183\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: take(): argument 'index' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# train_length      = round(0.7 * datalen)\n",
    "# test_length       = round(0.3 * datalen)\n",
    "\n",
    "## Loading images\n",
    "images, target = loader(PATH)\n",
    "## Train test split\n",
    "train_X, test_X, train_Y, test_Y = data_split(images, target, test_size = 0.3) \n",
    "## Train loader \n",
    "trainloader = create_batch(train_X, train_Y, batch_size = BATCHSIZE)\n",
    "## Test loader\n",
    "testloader = create_batch(test_X, test_Y, batch_size = BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1,  ..., 2, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9059, 0.8314, 0.6118,  ..., 0.1725, 0.1059, 0.0627],\n",
       "          [0.8980, 0.6863, 0.5804,  ..., 0.1059, 0.1373, 0.1059],\n",
       "          [0.8392, 0.6118, 0.6157,  ..., 0.1098, 0.0863, 0.1098],\n",
       "          ...,\n",
       "          [0.9451, 0.9608, 0.9804,  ..., 0.2235, 0.3961, 0.5725],\n",
       "          [0.9647, 0.9804, 0.9882,  ..., 0.3059, 0.4118, 0.6353],\n",
       "          [0.9804, 0.9843, 0.9804,  ..., 0.3451, 0.4314, 0.5961]]],\n",
       "\n",
       "\n",
       "        [[[0.2157, 0.2157, 0.2157,  ..., 0.4549, 0.3882, 0.2902],\n",
       "          [0.2157, 0.2157, 0.2157,  ..., 0.4941, 0.4510, 0.3529],\n",
       "          [0.2157, 0.2157, 0.2157,  ..., 0.5020, 0.5333, 0.4471],\n",
       "          ...,\n",
       "          [0.2196, 0.2196, 0.2196,  ..., 0.1176, 0.1216, 0.1529],\n",
       "          [0.2235, 0.2235, 0.2235,  ..., 0.1255, 0.1216, 0.2000],\n",
       "          [0.2235, 0.2235, 0.2235,  ..., 0.1333, 0.1176, 0.2235]]],\n",
       "\n",
       "\n",
       "        [[[0.0784, 0.0667, 0.0745,  ..., 0.7333, 0.6902, 0.6353],\n",
       "          [0.0863, 0.0667, 0.0667,  ..., 0.7647, 0.7059, 0.6706],\n",
       "          [0.0667, 0.0667, 0.0706,  ..., 0.7961, 0.7569, 0.6863],\n",
       "          ...,\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.4157, 0.4510, 0.4667],\n",
       "          [0.0078, 0.0078, 0.0039,  ..., 0.4039, 0.4353, 0.4667],\n",
       "          [0.0078, 0.0078, 0.0078,  ..., 0.3882, 0.4196, 0.4627]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.3294, 0.3333, 0.3333,  ..., 0.3373, 0.3373, 0.3412],\n",
       "          [0.3333, 0.3333, 0.3333,  ..., 0.3373, 0.3412, 0.3412],\n",
       "          [0.3333, 0.3333, 0.3373,  ..., 0.3373, 0.3412, 0.3412],\n",
       "          ...,\n",
       "          [0.4588, 0.4667, 0.3843,  ..., 0.8471, 0.7608, 0.7412],\n",
       "          [0.4196, 0.4157, 0.2980,  ..., 0.9255, 0.8392, 0.7647],\n",
       "          [0.4196, 0.3725, 0.2588,  ..., 0.9059, 0.8431, 0.7843]]],\n",
       "\n",
       "\n",
       "        [[[0.2902, 0.3176, 0.3412,  ..., 0.7412, 0.7490, 0.7529],\n",
       "          [0.3059, 0.3216, 0.3490,  ..., 0.7255, 0.7412, 0.7569],\n",
       "          [0.3176, 0.3373, 0.3686,  ..., 0.6902, 0.7255, 0.7569],\n",
       "          ...,\n",
       "          [0.3529, 0.3882, 0.4431,  ..., 0.7529, 0.7647, 0.7725],\n",
       "          [0.3451, 0.3765, 0.4471,  ..., 0.7529, 0.7608, 0.7529],\n",
       "          [0.3451, 0.3804, 0.4314,  ..., 0.7373, 0.7333, 0.7333]]],\n",
       "\n",
       "\n",
       "        [[[0.7647, 0.7804, 0.8039,  ..., 0.7137, 0.5490, 0.3020],\n",
       "          [0.7569, 0.7686, 0.7922,  ..., 0.7725, 0.6471, 0.4118],\n",
       "          [0.7765, 0.7843, 0.8000,  ..., 0.8157, 0.7686, 0.6157],\n",
       "          ...,\n",
       "          [0.2235, 0.2863, 0.3294,  ..., 0.0118, 0.0784, 0.1294],\n",
       "          [0.2392, 0.2863, 0.3765,  ..., 0.0235, 0.0745, 0.1608],\n",
       "          [0.2392, 0.3098, 0.3725,  ..., 0.0235, 0.0588, 0.1490]]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used 3 types of model\n",
    "\n",
    "    1. RESNET101\n",
    "    2. VGG19\n",
    "    3. XGBOOST\n",
    "\n",
    "Finally used bagging to ensemble those models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient freezer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RESNET101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESNET(nn.Module):\n",
    "    def __init__(self, criterion = None, optimizer = None, learning_rate = 0.001, image_dimention = 1, categories = 3):\n",
    "        super(RESNET, self).__init__()\n",
    "        ## Defining networt\n",
    "         # Defaulf input image dimention is 1\n",
    "         # Default output categories is 3\n",
    "        self.pretrained = models.resnet101(pretrained = True)\n",
    "        self.pretrained.conv1 = nn.Conv2d(image_dimention, 64, kernel_size = (3, 3), stride=(2,2), padding=(3,3), bias=False)\n",
    "        num_ftrs = self.pretrained.fc.in_features\n",
    "        self.pretrained.fc = nn.Linear(num_ftrs, categories)\n",
    "        \n",
    "        ## Defining optimizer and loss function\n",
    "         # Default loss function is cross entropy\n",
    "         # Default optimizer is SGD\n",
    "         # Default learning rate is 0.001\n",
    "        if criterion:\n",
    "            self.criterion = criterion\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = optim.SGD(self.pretrained.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def train(self, traindata, valdata = None, numberEpoch = 10, DEBUG = True):\n",
    "        trainlen = sum(list(batch[0].shape[0] for batch in traindata))\n",
    "        total_batch = len(traindata)\n",
    "        ## Loop over the dataset multiple times\n",
    "        for epoch in range(numberEpoch): \n",
    "            running_corrects = 0.0\n",
    "            running_loss     = 0.0\n",
    "            if DEBUG:\n",
    "                pbar = tqdm(enumerate(traindata, 0), total = total_batch, desc = \"Loss 0, Completed\", ncols = 800)\n",
    "            if not DEBUG:\n",
    "                pbar = enumerate(traindata, 0)\n",
    "            for count, data in pbar:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                batch  = inputs.shape[0]\n",
    "                inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "                \n",
    "                ## zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                ## forward + backward + optimize\n",
    "                outputs = self.forward(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Calculating statistics\n",
    "                running_loss += loss.item() * batch\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                ## Showing statistics\n",
    "                if DEBUG:\n",
    "                    pbar.set_description(\"Loss %.3f, Completed\" %(running_loss/trainlen))\n",
    "            if DEBUG:\n",
    "                epoch_loss = running_loss/trainlen\n",
    "                epoch_acc  = running_corrects/trainlen\n",
    "                print('Epoch %d completed, average loss: %.3f, accuracy: %.3f' %(epoch + 1, epoch_loss, epoch_acc))\n",
    "            \n",
    "                if valdata:\n",
    "                    val_loss, val_acc = self.evaluate(valdata)\n",
    "                    print('Validation, average loss: %.3f, accuracy: %.3f' %(val_loss, val_acc))\n",
    "                \n",
    "    def evaluate(self, testdata):\n",
    "        running_corrects = 0.0\n",
    "        running_loss     = 0.0\n",
    "        testlen = sum(list(batch[0].shape[0] for batch in testdata))\n",
    "        for data in testdata:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            batch  = inputs.shape[0]\n",
    "            inputs = inputs.type(torch.cuda.FloatTensor)            \n",
    "            ## Forward\n",
    "            outputs = self.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            ## Loss and accuracy\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            running_loss += loss.item() * batch\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        loss = running_loss/testlen\n",
    "        acc  = running_corrects/testlen\n",
    "        return loss, acc\n",
    "        \n",
    "    def predict(self, testdata, ID = None):\n",
    "        predicted_labels = []\n",
    "        for data in testdata:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            batch  = inputs.shape[0]\n",
    "            inputs = inputs.type(torch.cuda.FloatTensor)            \n",
    "            ## Forward\n",
    "            outputs = self.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predicted_labels += preds.tolist()\n",
    "        if ID:\n",
    "            return([ID[label] for label in predicted_labels])\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNET(nn.Module):\n",
    "    def __init__(self, criterion = None, optimizer = None, learning_rate = 0.001, image_dimention = 1, categories = 3):\n",
    "        super(VGGNET, self).__init__()\n",
    "        ## Defining networt\n",
    "         # Defaulf input image dimention is 1\n",
    "         # Default output categories is 3\n",
    "        self.pretrained = models.vgg19(pretrained = True)\n",
    "        self.pretrained.features[0] = nn.Conv2d(image_dimention, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        num_ftrs = self.pretrained.classifier[6].in_features\n",
    "        self.pretrained.classifier[6] = nn.Linear(num_ftrs, categories)\n",
    "        \n",
    "        ## Defining optimizer and loss function\n",
    "         # Default loss function is cross entropy\n",
    "         # Default optimizer is SGD\n",
    "         # Default learning rate is 0.001\n",
    "        if criterion:\n",
    "            self.criterion = criterion\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = optim.SGD(self.pretrained.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def train(self, traindata, valdata = None, numberEpoch = 10, DEBUG = True):\n",
    "        \n",
    "        trainlen = sum(list(batch[0].shape[0] for batch in traindata))\n",
    "        total_batch = len(traindata)\n",
    "        ## Loop over the dataset multiple times\n",
    "        for epoch in range(numberEpoch): \n",
    "            running_corrects = 0.0\n",
    "            running_loss     = 0.0\n",
    "            if DEBUG:\n",
    "                pbar = tqdm(enumerate(traindata, 0), total = total_batch, desc = \"Loss 0, Completed\", ncols = 800)\n",
    "            else:\n",
    "                pbar = enumerate(traindata, 0)\n",
    "            for count, data in pbar:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                batch  = inputs.shape[0]\n",
    "                inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "                \n",
    "                ## zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                ## forward + backward + optimize\n",
    "                outputs = self.forward(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                ## Calculating statistics\n",
    "                running_loss += loss.item() * batch\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                ## Showing statistics\n",
    "                if DEBUG:\n",
    "                    pbar.set_description(\"Loss %.3f, Completed\" %(running_loss/trainlen))\n",
    "            if DEBUG:\n",
    "                epoch_loss = running_loss/trainlen\n",
    "                epoch_acc  = running_corrects/trainlen\n",
    "                print('Epoch %d completed, average loss: %.3f, accuracy: %.3f' %(epoch + 1, epoch_loss, epoch_acc))\n",
    "            \n",
    "                if valdata:\n",
    "                    val_loss, val_acc = self.evaluate(valdata)\n",
    "                    print('Validation, average loss: %.3f, accuracy: %.3f' %(val_loss, val_acc))\n",
    "                \n",
    "    def evaluate(self, testdata):\n",
    "        running_corrects = 0.0\n",
    "        running_loss     = 0.0\n",
    "        testlen = sum(list(batch[0].shape[0] for batch in testdata))\n",
    "        with torch.no_grad():\n",
    "            for data in testdata:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                batch  = inputs.shape[0]\n",
    "                inputs = inputs.type(torch.cuda.FloatTensor)            \n",
    "                ## Forward\n",
    "                outputs = self.forward(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                ## Loss and accuracy\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item() * batch\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        loss = running_loss/testlen\n",
    "        acc  = running_corrects/testlen\n",
    "        return loss, acc\n",
    "        \n",
    "    def predict(self, testdata, ID = None):\n",
    "        predicted_labels = []\n",
    "        for data in testdata:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            batch  = inputs.shape[0]\n",
    "            inputs = inputs.type(torch.cuda.FloatTensor)            \n",
    "            ## Forward\n",
    "            outputs = self.forward(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predicted_labels += preds.tolist()\n",
    "        if ID:\n",
    "            return([ID[label] for label in predicted_labels])\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB = XGBClassifier(max_depth = 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers::  10\n",
      "Layers Freezed  ::  0\n"
     ]
    }
   ],
   "source": [
    "model_resnet = RESNET()\n",
    "model_resnet = model_resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.train(trainloader, valdata = testloader, numberEpoch = 25, DEBUG = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = VGGNET()\n",
    "model_vgg = model_vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.train(trainloader, valdata = testloader, numberEpoch = 25, DEBUG = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGB.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resnet\n",
    "prediction_resnet = model_resnet.predict(testloader)\n",
    "## VGG\n",
    "prediction_vgg    = model_vgg.predict(testloader)\n",
    "## XGB\n",
    "prediction_xgb    = model_XGB.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
